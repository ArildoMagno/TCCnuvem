<!--?xml version="1.0" encoding="utf-8" ?-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="Docutils 0.3.10: http://docutils.sourceforge.net/">
<title>Fundamentos de processamento lingüístico: tokenização de textos e classificação de palavras</title>
<meta name="copyright" content="© 2001-2006 University of Pennsylvania">
<style type="text/css">

/*
:Author: David Goodger
:Contact: goodger@users.sourceforge.net
:Date: $Date: 2005/06/13 20:15:52 $
:Version: $Revision: 1.1 $
:Copyright: This stylesheet has been placed in the public domain.

Default cascading style sheet for the HTML output of Docutils.
*/

/* "! important" is used here to override other ``margin-top`` and
   ``margin-bottom`` styles that are later in the stylesheet or 
   more specific.  See http://www.w3.org/TR/CSS1#the-cascade */
.first {
  margin-top: 0 ! important }

.last, .with-subtitle {
  margin-bottom: 0 ! important }

.hidden {
  display: none }

a.toc-backref {
  text-decoration: none ;
  color: black }

blockquote.epigraph {
  margin: 2em 5em ; }

dl.docutils dd {
  margin-bottom: 0.5em }

/* Uncomment (and remove this text!) to get bold-faced definition list terms
dl.docutils dt {
  font-weight: bold }
*/

div.abstract {
  margin: 2em 5em }

div.abstract p.topic-title {
  font-weight: bold ;
  text-align: center }

div.admonition, div.attention, div.caution, div.danger, div.error,
div.hint, div.important, div.note, div.tip, div.warning {
  margin: 2em ;
  border: medium outset ;
  font-size: smaller ;
  padding: 1em }

div.admonition p.admonition-title, div.hint p.admonition-title,
div.important p.admonition-title, div.note p.admonition-title,
div.tip p.admonition-title {
  font-weight: bold ;
  font-size: smaller ;
  font-family: sans-serif }

div.attention p.admonition-title, div.caution p.admonition-title,
div.danger p.admonition-title, div.error p.admonition-title,
div.warning p.admonition-title {
  color: red ;
  font-weight: bold ;
  font-family: sans-serif }

/* Uncomment (and remove this text!) to get reduced vertical space in
   compound paragraphs.
div.compound .compound-first, div.compound .compound-middle {
  margin-bottom: 0.5em }

div.compound .compound-last, div.compound .compound-middle {
  margin-top: 0.5em }
*/

div.dedication {
  margin: 2em 5em ;
  text-align: center ;
  font-style: italic }

div.dedication p.topic-title {
  font-weight: bold ;
  font-style: normal }

div.figure {
  margin-left: 2em }

div.footer, div.header {
  clear: both;
  font-size: smaller }

div.line-block {
  display: block ;
  margin-top: 1em ;
  margin-bottom: 1em }

div.line-block div.line-block {
  margin-top: 0 ;
  margin-bottom: 0 ;
  margin-left: 1.5em }

div.sidebar {
  margin-left: 1em ;
  border: medium outset ;
  padding: 1em ;
  background-color: #ffffee ;
  width: 40% ;
  float: right ;
  clear: right }

div.sidebar p.rubric {
  font-family: sans-serif ;
  font-size: medium }

div.system-messages {
  margin: 5em }

div.system-messages h1 {
  color: red }

div.system-message {
  border: medium outset ;
  padding: 1em }

div.system-message p.system-message-title {
  color: red ;
  font-weight: bold }

div.topic {
  margin: 2em }

h1.section-subtitle, h2.section-subtitle, h3.section-subtitle,
h4.section-subtitle, h5.section-subtitle, h6.section-subtitle {
  margin-top: 0.4em }

h1.title {
  text-align: center }

h2.subtitle {
  text-align: center }

hr.docutils {
  width: 75% }

img.align-left {
  clear: left }

img.align-right {
  clear: right }

img.borderless {
  border: 0 }

ol.simple, ul.simple {
  margin-bottom: 1em }

ol.arabic {
  list-style: decimal }

ol.loweralpha {
  list-style: lower-alpha }

ol.upperalpha {
  list-style: upper-alpha }

ol.lowerroman {
  list-style: lower-roman }

ol.upperroman {
  list-style: upper-roman }

p.attribution {
  text-align: right ;
  margin-left: 50% }

p.caption {
  font-style: italic }

p.credits {
  font-style: italic ;
  font-size: smaller }

p.label {
  white-space: nowrap }

p.rubric {
  font-weight: bold ;
  font-size: larger ;
  color: maroon ;
  text-align: center }

p.sidebar-title {
  font-family: sans-serif ;
  font-weight: bold ;
  font-size: larger }

p.sidebar-subtitle {
  font-family: sans-serif ;
  font-weight: bold }

p.topic-title {
  font-weight: bold }

pre.address {
  margin-bottom: 0 ;
  margin-top: 0 ;
  font-family: serif ;
  font-size: 100% }

pre.line-block {
  font-family: serif ;
  font-size: 100% }

pre.literal-block, pre.doctest-block {
  margin-left: 2em ;
  margin-right: 2em ;
  background-color: #eeeeee }

span.classifier {
  font-family: sans-serif ;
  font-style: oblique }

span.classifier-delimiter {
  font-family: sans-serif ;
  font-weight: bold }

span.interpreted {
  font-family: sans-serif }

span.option {
  white-space: nowrap }

span.pre {
  white-space: pre }

span.problematic {
  color: red }

span.section-subtitle {
  /* font-size relative to parent (h1..h6 element) */
  font-size: 80% }

table.citation {
  border-left: solid thin gray }

table.docinfo {
  margin: 2em 4em }

table.docutils {
  margin-top: 0.5em ;
  margin-bottom: 0.5em }

table.footnote {
  border-left: solid thin black }

table.docutils td, table.docutils th,
table.docinfo td, table.docinfo th {
  padding-left: 0.5em ;
  padding-right: 0.5em ;
  vertical-align: top }

table.docutils th.field-name, table.docinfo th.docinfo-name {
  font-weight: bold ;
  text-align: left ;
  white-space: nowrap ;
  padding-left: 0 }

h1 tt.docutils, h2 tt.docutils, h3 tt.docutils,
h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {
  font-size: 100% }

/*
tt.docutils {
  background-color: #eeeeee }
*/

tt.docutils {
  font-size: large }

ul.auto-toc {
  list-style-type: none }

</style>
</head>
<body>
<div class="document" id="fundamentos-de-processamento-ling-stico-tokeniza-o-de-textos-e-classifica-o-de-palavras">
<h1 class="title">Fundamentos de processamento lingüístico: tokenização de textos e classificação de palavras</h1>
<table class="docinfo" frame="void" rules="none">
<colgroup><col class="docinfo-name">
<col class="docinfo-content">
</colgroup><tbody valign="top">
<tr class="field"><th class="docinfo-name">Autores:</th><td class="field-body">Steven Bird, Ewan Klein, Edward Loper</td>
</tr>
<tr class="field"><th class="docinfo-name">Contato:</th><td class="field-body"><a class="reference" href="mailto:sb@csse.unimelb.edu.au">sb@csse.unimelb.edu.au</a></td>
</tr>
<tr class="field"><th class="docinfo-name">Versão:</th><td class="field-body">0.6.3</td>
</tr>
<tr class="field"><th class="docinfo-name">Revisão:</th><td class="field-body">1.7</td>
</tr>
<tr class="field"><th class="docinfo-name">Data:</th><td class="field-body">2006-01-29</td>
</tr>
<tr><th class="docinfo-name">Copyright:</th>
<td>© 2001-2006 University of Pennsylvania</td></tr>
<tr class="field"><th class="docinfo-name">Licença:</th><td class="field-body">Creative Commons Attribution-ShareAlike License</td>
</tr>
</tbody>
</table>
<!-- -*- mode: rst -*- -->
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last">Este é um esboço. Por favor, envie seus comentários aos autores.</p>
</div>
<div class="section">
<h1><a id="id1" name="id1"><span id="introdu-o"></span>1&nbsp;&nbsp;&nbsp;Introdução</a></h1>
<p>Textos são geralmente representados em computadores por meio de arquivos que 
contêem uma seqüência potencialmente longa de caracteres. Para a maioria dos tipos
de processamento lingüístico é necessário identificar e categorizar as palavras de
um texto. Esta se revela uma tarefa nada trivial. Neste capítulo iremos introduzir
os <em>toquens</em> como sendo os blocos constituíntes dos textos e mostraremos de que
forma estes últimos podem ser <em>toquenizados</em>. Em seguida, iremos considerar a
categorização dos tokens de acordo com suas funções como parte-do-discurso, além
de realizar uma exploração preliminar do Brown Corpus, uma conjunto de mais de
um milhão de palavras em língua inglesa com tags (informações quanto à
categorização das mesmas). Durante o capítulo serão apresentadas algumas
aplicações interessantes: geração de textos aleatórios, classificação automática
de palavras e análise dos verbos modais em textos de diferentes gêneros
(sempre em língua inglesa).</p>
</div>
<div class="section">
<h1><a id="toquens-os-blocos-constitu-ntes-de-um-texto" name="toquens-os-blocos-constitu-ntes-de-um-texto">2&nbsp;&nbsp;&nbsp;Toquens: os blocos constituíntes de um texto</a></h1>
<p>Como sabemos que determinada parte de um texto constitui uma <em>palavra</em> e de
qual forma podemos representar estas e as informações a elas associadas em
uma máquina? Pode parecer excessivamente pedante exigir uma definição para
"palavra"; não podemos dizer simplesmente que trata-se de uma seqüência
de caracteres com um espaço no início e outro fim? Com o evoluir do estudo,
a questão revela-se um pouco mais complexa. Para termos uma noção prática
dos problemas, vamos considerar o seguinte texto retirado do Wall Street
Journal:</p>
<pre class="literal-block">Parágrafo 12 do arquivo ``wsj_0034``

It's probably worth paying a premium for funds that invest in markets
that are partially closed to foreign investors, such as South Korea,
some specialists say.  But some European funds recently have
skyrocketed; Spain Fund has surged to a startling 120% premium.  It has
been targeted by Japanese investors as a good long-term play tied to
1992's European economic integration.  And several new funds that aren't
even fully invested yet have jumped to trade at big premiums.

"I'm very alarmed to see these rich valuations," says Smith Barney's
Mr. Porter.
</pre>
<p>Vamos começar pela string <tt class="docutils literal"><span class="pre">aren't</span></tt>. De acordo com nossa definição simplista,
esta constitui uma única palavra. Mas considere a possibilidade de
queremos verificar se todas as palavras de nosso texto são listadas em
um dicionário e nosso dicionário apresenta definições para <tt class="docutils literal"><span class="pre">are</span></tt> e
<tt class="docutils literal"><span class="pre">not</span></tt>, mas não para <tt class="docutils literal"><span class="pre">aren't</span></tt>. Neste caso, seria muito mais favorável
à nossa pesquisa reconhecer em <tt class="docutils literal"><span class="pre">aren't</span></tt> uma contração de duas palavras
distintas.</p>
<!-- Podemos apresentar uma argumentação semelhante a respeito de ``1992's``.
Poderíamos querer executar um simples programa que extraísse de nosso
texto todas as palavras que representem datas. In this case, we would
get achieve more generality by first stripping oexcept in this case,
we would not expect to find ``1992`` in a dictionary. -->
<p>Se tomarmos nossa definição simplista de palavra literalmente (como
deveríamos fazer, caso estejamos pensando em implementá-la em código), nos
depararemos com outros menores mas sempre reais problemas. Por exemplo,
assumindo que nosso arquivo seja constituído por linhas separadas, como no
texto do WSJ acima, então todas as palavras que estiverem no início de uma
linha não serão reconhecidas devido à ausência de um espaço antes destas
(a menos que consideremos o caractere de "nova linha" como um espaço). Segundo,
de acordo com nosso critério, símbolos de pontuação irão fazer parte das
palavras; ou seja, uma string como <tt class="docutils literal"><span class="pre">investors,</span></tt> também será considerada
uma palavra, pois não há nenhum espaço entre <tt class="docutils literal"><span class="pre">investors</span></tt> e a vírgula que
a segue. Conseqüentemente, corremos o risco de não conseguir reconhecer em
<tt class="docutils literal"><span class="pre">investors,</span></tt> (com a vírgula final) um toquen do mesmo tipo de <tt class="docutils literal"><span class="pre">investors</span></tt>
(sem a vírgula final). Mais importante, seria interessante que os sinais de
pontuação funcionassem como "first-class citizen" para a toquenização e a
para o processamento subseqüente. Por exemplo, podemos desejar implementar
uma regra que especifique que uma palavra seguida por um ponto é
provavelmente uma abreviatura, caso a palavra que a siga inicie-se por
letra minúscula. Porém, para formular este tipo de regra, é necessário que
possamos identificar o ponto como um toquen isolado.</p>
<p>Um desafio um pouco diferente é apresentado por exemplos como os
seguintes (provenientes do corpus MedLine [ref]):</p>
<ol class="arabic simple">
<li>This is a alpha-galactosyl-1,4-beta-galactosyl-specific adhesin.</li>
<li>The corresponding free cortisol fractions in these sera were 4.53
+/- 0.15% and 8.16 +/- 0.23%, respectively.</li>
</ol>
<p>Em casos como estes, nos deparamos com termos que dificilmente seriam
encontrados em qualquer dicionário ou vocabulário não setorial da língua
inglesa. Além disso, não teríamos sucesso ao tentar analisar sintaticamente
estas strings utilizando uma gramática padrão do inglês. Para determinadas
finalidades, seria conveniente "agrupar" expressões como
<tt class="docutils literal"><span class="pre">alpha-galactosyl-1,4-beta-galactosyl-specific</span> <span class="pre">adhesin</span></tt> e <tt class="docutils literal"><span class="pre">4.53</span>
<span class="pre">+/-</span> <span class="pre">0.15%</span></tt> de forma a sereme reconhecidas pelo parser como átomos não
analisáveis. Para isto, deveremos tratá-las como "palavras" únicas para
qualquer finalidade dos processamentos subseqüentes. O resultado geral disto
tudo é que, mesmo que nos atenhamos a uma única língua como o inglês, a
questão de como definir uma palavra depende muito de quais são nossos
objetivos.</p>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last">Caso nos voltemos para outras línguas que não o inglês, a 
segmentação das palavras pode ser ainda mais desafiadora. Por exemplo,
na ortografia chinêsa os caracteres correspondem a morfemas
monosilábicos. Muitos morfemas constituem por si próprios uma palavra,
mas várias palavras contêem mais de um morfema; a maioria é constituída por
dois morfemas. Apesar disso, não há nenhuma representação visual dos
limites das palavras em um texto em chinês.</p>
</div>
<p>Vamos dar uma olhada mais detalhada às palavras do texto do WSJ. Suponhamos
que os espaços venham a ser utilizados como delimitadores de palavras e que a
lista de todas as palavras do texto tenha sido colocada em ordem
alfabética; podemos esperar por um resultado como o seguinte:</p>
<pre class="literal-block">120, 1992, And, Barney, But, European, European, Fund, I, It, It,
Japanese, Korea, Mr, Porter, Smith, South, Spain, a, a, a, ...
</pre>
<p>Se a esta altura pedirmos a um programa que nos informe quantas palavras
há no texto, ele provavelmente fornecerá como resposta 90. Esta resposta
deve-se ao fato de que cada uma das três ocorrências da palavra <tt class="docutils literal"><span class="pre">a</span></tt> está
sendo considerada uma palavra isolada. Mas o que significa dizer que
determinado objeto <tt class="docutils literal"><span class="pre">a</span></tt> ocorre três vezes? Estamos na presença de três
palavras <tt class="docutils literal"><span class="pre">a</span></tt> ou de uma única palavra repetida três vezes? Podemos responder
"ambos os casos", se traçarmos uma distinção entre um <em>toquen</em> de palavra e
um <em>tipo</em> de palavra. Um tipo de palavra é algo abstrato; é aquilo ao qual
nos referimos quando dizemos que conhecemos o significado da palavra
<tt class="docutils literal"><span class="pre">deprecate</span></tt> ou quando dizemos que as palavras <tt class="docutils literal"><span class="pre">barf</span></tt> e <tt class="docutils literal"><span class="pre">vomit</span></tt> são
sinônimos. Por outro lado, um token de palavra é algo que existe no tempo e
no espaço. Por exemplo, podemos nos referir à palavra <tt class="docutils literal"><span class="pre">grunge</span></tt> que pronunciei
em Edinburgo no dia 14 de julho de 2003; da mesma forma, podemos dizer que o
segundo toquen do texto do WSJ é um toquen de tipo de palavra <tt class="docutils literal"><span class="pre">probably</span></tt> ou
que há dois toquens do tipo <tt class="docutils literal"><span class="pre">European</span></tt> no texto. Em termos mais gerais,
podemos dizer que há 90 toquens de palavras no texto do WSJ mas apenas 76
tipos de palavras.</p>
<p>Os termos <em>toquen</em> e <em>tipo</em> também podem ser aplicados a outras entidades
lingüísticas. Por exemplo, um <em>toquen de sentença</em> é uma ocorrência individual
de uma determinada sentença; mas um <em>tipo de sentença</em> é uma sentença em
termos abstratos, sem contexto. Se alguém repete uma sentença, este alguém
pronunciou dois toquens de sentença, mas apenas um tipo de sentença foi
utilizado. Neste livro, quando a categoria de toquen ou tipo for óbvia a
partir do contexto, utilizaremos apenas os termos token e tipo.</p>
<div class="section">
<h2><a id="representando-toquens" name="representando-toquens"><span id="representing-tokens"></span>2.1&nbsp;&nbsp;&nbsp;Representando toquens</a></h2>
<p>Quando a linguagem escrita é armazenada em um arquivo de computador, ela
é normalmente representada por meio de uma seqüência ou <em>string</em> (do inglês,
"cadeia") de caracteres. Isto é, em um arquivo de texto padrão, as
palavras são strings, as sentenças são strings e o próprio texto não passa,
no fundo, de uma longa string. Os caracteres de uma string não precisam
ser necessariamente alfanuméricos; estas também podem incluir caracteres
especiais que representem os espaços, as tabulações, os sinais de nova
linha, etc.</p>
<p>Muito do processamento computacional é realizado acima do plano dos
caracteres. Quando compilamos uma linguagem de programação, por exemplo,
o compilador espera que o input (os dados de entrada) seja uma seqüência
de tokens com os quais ele seja capaz de lidar; por exemplo, as classes
dos identificadores, constantes textuais e alfanuméricas. De forma
análoga, um parser espera que seu input seja uma seqüência de tokens de
palavras e não uma seqüência de caracteres individuais. Em sua forma mais
simples, portanto, a toquenização de um texto envolve a busca das
posições da string que contêem os chamados "caracteres em branco" (espaços,
tabulações ou sinais de nova linha) ou sinais de pontuação específicos,
separando a string em toquens nestas posições. Por exemplo, suponhamos que
estejamos trabalhando com um arquivo que contenha as duas linhas abaixo:</p>
<pre class="literal-block">The cat climbed
the tree.
</pre>
<p>Do ponto de vista do parser, este arquivo é meramente uma string de
caracteres:</p>
<blockquote>
'The_cat_climbed\nthe_tree.'</blockquote>
<p>Note que utilizamos apóstrofos para deimitar as strings, "_" para
representar espaços e a expressão "n" para representar uma nova linha.</p>
<p>Como acabamos de dizer, para toquenizar este texto e permitir sua
utilização pelo parser é necessário indicar explicitamente quais substrings
são palavras. Uma forma conveniente de se fazer isto em Python é dividir a
string em uma <em>lista</em> de palavras, na qual cada palavra seja uma string, como
em <tt class="docutils literal"><span class="pre">'dog'</span></tt>. <a class="footnote-reference" href="#id3" id="id2" name="id2">[1]</a>
Em Python, uma lista é representada como uma série de objetos (neste caso,
strings) separados por vírgulas, delimitada por conchetes:</p>
<pre class="literal-block">&gt;&gt;&gt; palavras = ['the', 'cat', 'climbed', 'the', 'tree']
&gt;&gt;&gt; palavras
['the', 'cat', 'climbed', 'the', 'tree']
</pre>
<table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label"><col></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2" name="id3">[1]</a></td><td>Dissemos "conveniente" pois o Python é possível uma interação pela
lista, processando os seus elementos um a um.</td></tr>
</tbody>
</table>
<p>Note que introduzimos uma nova variável, chamada <tt class="docutils literal"><span class="pre">palavras</span></tt>, à qual
é atribuída a lista e que entramos com o nome da variável em uma nova
linha para verificar seu conteúdo.</p>
<p>Em resumo, acabamos de mostrar como, em sua forma mais simples, a toquenização
de um texto pode ser realizada convertendo uma única string representando o
texto em uma lista de strings, na qual cada elemento represente uma palavra.</p>
</div>
</div>
<div class="section">
<h1><a id="toqueniza-o" name="toqueniza-o">3&nbsp;&nbsp;&nbsp;Toquenização</a></h1>
<p>Muitas tarefas do processamento de linguagem natural envolvem a análise de
textos de dimensões variadas, indo de uma única sentença até corpora
extensos. Há várias formas de se representar textos usando o NLTK. A
mais simples destas é por meio de uma única string. Estas strings podem
ser lidas diretamente de arquivos:</p>
<blockquote>
<pre class="doctest-block">&gt;&gt;&gt; text_str = open('corpus.txt').read() 
&gt;&gt;&gt; text_str
'Hello world.  This is a test file.\n'
</pre>
</blockquote>
<p>Porém, como notamos em <a class="reference" href="#representing-tokens">representing.tokens</a>, é normalmente preferível
representar um texto como uma lista de toquens. Estas listas são normalmente
criadas utilizando-se um <em>toquenizador</em> como o <tt class="docutils literal"><span class="pre">tokenize.whitespace</span></tt> que
separa as strings em palavras conforme os espaços:</p>
<blockquote>
<pre class="doctest-block">&gt;&gt;&gt; from nltk_lite import tokenize
&gt;&gt;&gt; text = 'Hello world.  This is a test string.'
&gt;&gt;&gt; list(tokenize.whitespace(text))
['Hello', 'world.', 'This', 'is', 'a', 'test', 'string.']
</pre>
</blockquote>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last">Por "espaço", não entendemos apenas o espaço entre palavras, mas
também as tabulações e os finais de linha.</p>
</div>
<p>A toquenização pode normalizar um texto, por exemplo mapeando suas
palavras para versões apenas com letra minúscula, expandindo
contrações e até mesmo extraíndo o radical de cada palavra, processo
este conhecido por "stemming". Um exemplo de stemming é listado
abaixo:</p>
<blockquote>
<pre class="doctest-block">&gt;&gt;&gt; text = 'stemming can be fun and exciting'
&gt;&gt;&gt; tokens = tokenize.whitespace(text)
&gt;&gt;&gt; porter = tokenize.PorterStemmer()
&gt;&gt;&gt; for token in tokens:
...     print porter.stem(token),
stem can be fun and excit
</pre>
</blockquote>
<p>A toquenização baseada em espaços é simplista demais para a maioria das
aplicações; por exemplo, ela falha ao separar a última palavra de uma
frase ou sentença de caracteres de pontuação como a vírgula, o ponto
final, o ponto de exclamação e o ponto de interrogação. Como o nome
sugere, o <tt class="docutils literal"><span class="pre">tokenize.regexp</span></tt> utiliza uma expressão regular para
determinar de que forma um texto deve ser dividido. Esta expressão regular
define os caracteres que podem constituir uma palavra válida. Para
definir um toquenizador que inclua sinais de pontuação como toquens
separados, poderiamos utilizar:</p>
<pre class="literal-block">&gt;&gt;&gt; text = '''Hello.  Isn't this fun?'''
&gt;&gt;&gt; pattern = r'\w+|[^\w\s]+'
&gt;&gt;&gt; list(tokenize.regexp(text, pattern))
['Hello', '.', 'Isn', "'", 't', 'this', 'fun', '?']
</pre>
<div class="tip">
<p class="first admonition-title">Tip</p>
<p class="last">Lembre-se que <tt class="docutils literal"><span class="pre">\w+|[^\w\s]+</span></tt> é uma disjunção de duas
subexpressões, <tt class="docutils literal"><span class="pre">w+</span></tt> e <tt class="docutils literal"><span class="pre">[^\w\s]+</span></tt>. A primeira destas aceita um ou
mais caracteres "de palavra"; ou seja, caracteres que não sejam espaços
em branco ou sinais de pontuação. O segundo padrão é uma expressão negada;
ela aceita um ou mais caracteres que não sejam caracteres de palavra (ou
seja, não aceita o conjunto <tt class="docutils literal"><span class="pre">\w</span></tt>) ou espaços em branco (ou seja, não
aceita o conjunto <tt class="docutils literal"><span class="pre">\s</span></tt>).</p>
</div>
<div class="tip">
<p class="first admonition-title">Tip</p>
<p class="last">A expressão regular deste exemplo aceitará uma seqüência que
seja constituída por um ou mais caracteres de palavra <tt class="docutils literal"><span class="pre">\w+</span></tt>. Também 
aceitará uma seqüência que seja constituída por um ou mais sinais de
pontuação (ou caracteres não-palavra e não-espaço, <tt class="docutils literal"><span class="pre">[^\w\s]+</span></tt>).</p>
</div>
<p>Há várias formas por meio das quais podemos melhorar esta expressão regular.
Por exemplo, em sua versão atual ela separa a string <tt class="docutils literal"><span class="pre">'$22.50'</span></tt> em quatro
toquens distintos; podemos porém desejar que ela identifique neste tipo de
string um único toquen. A solução para isto é modificar a expressão regular,
adicionado uma nova subexpressão que trate especificamente de strings
neste formato:</p>
<pre class="literal-block">&gt;&gt;&gt; text = 'That poster costs $22.40.'
&gt;&gt;&gt; pattern = r'\w+|\$\d+\.\d+|[^\w\s]+'
&gt;&gt;&gt; list(tokenize.regexp(text, pattern))
['That', 'poster', 'costs', '$22.40', '.']
</pre>
<p>Certas vezes, é mais conveniente escrever uma expressão regular que
aceite o que é encontrado <em>entre</em> os tokens, como os espaços em branco ou
os sinais de pontuação. O construtor de função <tt class="docutils literal"><span class="pre">tokenize()</span></tt> aceita um
parâmetro booleano opcional <tt class="docutils literal"><span class="pre">gaps</span></tt> (em inglês, ---);  quando definido como
<tt class="docutils literal"><span class="pre">True</span></tt> (em inglês, verdadeiro) o padrão é verificado against the gaps.
Por exemplo, eis como o <tt class="docutils literal"><span class="pre">whitespaceTokenize</span></tt> é definido:</p>
<pre class="literal-block">&gt;&gt;&gt; list(tokenize.regexp(text, pattern=r'\s+', gaps=True))
['That', 'poster', 'costs', '$22.40.']
</pre>
<p>O package <tt class="docutils literal"><span class="pre">nltk_lite.corpora</span></tt> disponibiliza acesso imediato a vários
corpora distribuídos com o NLTK, juntamente com toquenizadores built-in.
Por exemplo, o <tt class="docutils literal"><span class="pre">brown.tagged()</span></tt> é um iterator sobre as sentenças com
tags do Brown Corpus. Utilizamos a função <tt class="docutils literal"><span class="pre">extract()</span></tt> para extrair
uma sentença que nos interesse:</p>
<pre class="literal-block">&gt;&gt;&gt; from nltk_lite.corpora import brown, extract
&gt;&gt;&gt; print extract(0, brown.tagged('a'))
[('The', 'at'), ('Fulton', 'np-tl'), ('County', 'nn-tl'), ('Grand', 'jj-tl'), ('Jury', 'nn-tl'), ('said', 'vbd'), ('Friday', 'nr'), ('an', 'at'), ('investigation', 'nn'), ('of', 'in'), ("Atlanta's", 'np$'), ('recent', 'jj'), ('primary', 'nn'), ('election', 'nn'), ('produced', 'vbd'), ('``', '``'), ('no', 'at'), ('evidence', 'nn'), ("''", "''"), ('that', 'cs'), ('any', 'dti'), ('irregularities', 'nns'), ('took', 'vbd'), ('place', 'nn'), ('.', '.')]
</pre>
<p>Em particular, o <tt class="docutils literal"><span class="pre">brown.tagged()</span></tt> não apenas carrega o texto relevante
em uma string, mas também utiliza o toquenizador apropriado.</p>
<!-- frequency: -->
</div>
<div class="section">
<h1><a id="contando-toquens" name="contando-toquens">4&nbsp;&nbsp;&nbsp;Contando toquens</a></h1>
<p>Provavelmente, a coisa mais fácil que se possa fazer com os toquens, uma
vez que tenham sido extraídos de um texto, é contá-los. Podemos fazer
isto como mostrado a seguir, quando comparamos o comprimento das traduções
para o inglês e para o finlandês do livro do Gênesis:</p>
<pre class="literal-block">&gt;&gt;&gt; from nltk_lite.corpora import genesis
&gt;&gt;&gt; len(list(genesis.raw('english-kjv')))
38240
&gt;&gt;&gt; len(list(genesis.raw('finnish')))
26595
</pre>
<p>Podemos realizar uma contagem mais sofisticada por meio de uma <em>distribuição
de freqüência</em>. Geralmente, uma distribuição de freqüência armazena o
número de vezes que cada determinado valor ocorre como resultado de uma
experiência. Neste caso, podemos utilizar uma distribuição de freqüência
para armazenar a freqüência de cada palavra em um documento. Distribuições
de freqüência são geralmente inicializadas executando-se repetidamente uma
experiência e incrementando a contagem de cada valor toda vez que este for
obtido como resultado. O programa a seguir cria uma distribuição de
freqüência que armazena o número de vezes que cada palavra ocorre em
um texto, exibindo ao final a palavra mais freqüente:</p>
<pre class="literal-block">&gt;&gt;&gt; from nltk_lite.probability import FreqDist
&gt;&gt;&gt; fd = FreqDist()
&gt;&gt;&gt; for token in genesis.raw():
...     fd.inc(token)
&gt;&gt;&gt; fd.max()
'the'
</pre>
<p>Uma vez que tenhamos construído uma distribuição de freqüência que
armazene os resultados de uma experiência, podemos utilizá-la para examinar
uma série de propriedades interessantes desta experiência. Estas
propriedades são resumidas abaixo:</p>
<table class="docutils" border="1">
<colgroup>
<col width="13%">
<col width="23%">
<col width="64%">
</colgroup>
<thead valign="bottom">
<tr><th class="head" colspan="3">Módulo de distribuição de freqüência</th>
</tr>
<tr><th class="head">Nome</th>
<th class="head">Código de exemplo</th>
<th class="head">Descrição</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>Contagem</td>
<td>fd.count('the')</td>
<td>número de vezes que determinado resultado ocorreu</td>
</tr>
<tr><td>Freqüência</td>
<td>fd.freq('the')</td>
<td>freqüência de um dado resultado</td>
</tr>
<tr><td>N</td>
<td>fd.N()</td>
<td>número de resultados</td>
</tr>
<tr><td>Resultados</td>
<td>fd.samples()</td>
<td>lista com cada resultado distinto armazenado</td>
</tr>
<tr><td>Max</td>
<td>fd.max()</td>
<td>resultado com maior número de ocorrência</td>
</tr>
</tbody>
</table>
<p>Podemos utilizar uma <tt class="docutils literal"><span class="pre">FreqDist</span></tt> para examinar a distribuição do
comprimento das palavras em um determinado corpus. Para cada palavra,
encontramos seu comprimento e incrementamos a contagem para as palavras
com aquele comprimento.</p>
<blockquote>
<pre class="doctest-block">&gt;&gt;&gt; def length_dist(text):
...     fd = FreqDist()                        # inicializa uma distribuição de freqüência vazia
...     for token in genesis.raw(text):        # para cada toquen
...         fd.inc(len(token))                 # encontrada outra palavra com este comprimento
...     for i in range(15):                    # para cada comprimento entre 0 e 14
...         print "%2d" % int(100*fd.freq(i)), # exibe a porcentagem de palavras com este comprimento
...     print
</pre>
<pre class="doctest-block">&gt;&gt;&gt; length_dist('english-kjv')
 0  2 14 28 21 13  7  5  2  2  0  0  0  0  0
&gt;&gt;&gt; length_dist('finnish')
 0  0  9  6 11 16 16 12  9  6  3  2  2  1  0
</pre>
</blockquote>
<p>Uma <em>condição</em> especifica o contexto no qual uma experiência foi
realizada. Normalmente nos interessaremos pelos efeitos que as condições
produzem no resultado de uma experiência. Por exemplo, podemos querer
examinar de que forma a distribuição do comprimento das palavras (o
resultado) é influenciado pela letra inicial das mesmas (a condição).
Distribuições de freqüência condicionais fornecem uma ferramenta para
explorar este tipo de questão.</p>
<p>Uma <em>distribuição de freqüência condicional</em> é um conjunto de distribuições
de freqüência para uma mesma experiência, executada sob condições
diferentes. As distribuições de freqüência individuais são indexadas
por sua condição.</p>
<pre class="literal-block">&gt;&gt;&gt; from nltk_lite.probability import ConditionalFreqDist
&gt;&gt;&gt; cfdist = ConditionalFreqDist()

&gt;&gt;&gt; for text in genesis.items:
...     for word in genesis.raw(text):
...         cfdist[text].inc(len(word))
</pre>
<p>Para plotar os resultados, construímos uma lista de pontos, onde a
coordenada x refere-se ao comprimento das palavras e a coordenada y à
freqüência de cada um destes comprimentos:</p>
<blockquote>
<pre class="doctest-block">&gt;&gt;&gt; for cond in cfdist.conditions():
...     wordlens = cfdist[cond].samples()
...     wordlens.sort()
...     points = [(i, cfdist[cond].freq(i)) for i in wordlens]
</pre>
</blockquote>
<p>Podemos plotar estes pontos usando a função <tt class="docutils literal"><span class="pre">Plot</span></tt> definida em
<tt class="docutils literal"><span class="pre">nltk_lite.draw.plot</span></tt>, da seguinte forma: <tt class="docutils literal"><span class="pre">Plot(points).mainloop()</span></tt>.</p>
</div>
<div class="section">
<h1><a id="uma-aplica-o-pr-tica-prever-a-palavra-sucessiva" name="uma-aplica-o-pr-tica-prever-a-palavra-sucessiva">5&nbsp;&nbsp;&nbsp;Uma aplicação prática: prever a palavra sucessiva</a></h1>
<p>Distribuições de freqüência condicionais são muitas vezes utilizadas
para prever certos comportamentos. <em>Prever</em>, neste sentido, significa
estimar um valor plausível como resultado da execução de determinada
experiência. A decisão de qual resultado fornecer é normalmente uma função
do contexto no qual a experiência é realizada. Por exemplo, podemos
tentar prever uma palavra de um texto (o resultado) baseado-nos no texto
que antecede esta palavra (o contexto).</p>
<p>Para prever os resultados de uma experiência, é necessário que,
primeiramente, examinemos um <em>corpus de treinamento</em> representativo, no
qual o contexto e o resultado para cada execução da experiência já são
conhecidos. Quando apresentados a uma nova execução da experiência,
simplesmente escolhemos o resultado que ocorreu mais freqüentemente
naquele contexto específico.</p>
<p>Podemos utilizar uma <tt class="docutils literal"><span class="pre">ConditionalFreqDist</span></tt> para encontrar a ocorrência
mais freqüente de cada contexto. Primeiro, armazenamos cada resultado do
corpus de treinamento, utilizando o contexto no qual a experiência é
executada como condição. Então, podemos acessar a freqüência de distribuição
para cada dado contexto usando este último como índice e, finalmente,
utilizar o método <tt class="docutils literal"><span class="pre">max()</span></tt> para encontrar o resultado mais provável.</p>
<p>Abaixo, usaremos uma <tt class="docutils literal"><span class="pre">ConditionalFreqDist</span></tt> para encontrar a palavra mais
provável em um texto. Para iniciar, carregaremos um corpus a partir de um
arquivo de texto e criaremos uma <tt class="docutils literal"><span class="pre">ConditionalFreqDist</span></tt> vazia:</p>
<pre class="literal-block">&gt;&gt;&gt; from nltk_lite.probability import ConditionalFreqDist
&gt;&gt;&gt; cfdist = ConditionalFreqDist()
</pre>
<p>A seguir, examinaremos cada toquen neste corpus, incrementando a
contagem apropriada para cada um. A variável <tt class="docutils literal"><span class="pre">prev</span></tt> será utilizada para
armazenar a palavra anterior.</p>
<blockquote>
<pre class="doctest-block">&gt;&gt;&gt; prev = None
&gt;&gt;&gt; for word in genesis.raw():
...     cfdist[prev].inc(word)
...     prev = word
</pre>
</blockquote>
<div class="note">
<p class="first admonition-title">Note</p>
<p class="last">Certas vezes, o contexto de uma experiência pode não estar
disponível ou não existir. Por exemplo, no caso do primeiro toquen não
há nenhum texto que o anteceda. Em situações como esta, somos obrigados
a decidir qual contexto utilizar. Neste exemplo, utilizamos o <tt class="docutils literal"><span class="pre">None</span></tt>
(do inglês, "nada") como contexto para o primeiro toquen. Outra
opção teria sido descartar as informações do primeiro toquen.</p>
</div>
<p>Uma vez que tenhamos construído uma distribuição de freqüência condicional
para o corpus de treinamento, podemos utilizá-la para encontrar a palavra
mais provável para um dado contexto. Por exemplo, tomando por contexto a
palavra <tt class="docutils literal"><span class="pre">living</span></tt>, é possível inspecionar todas as palavras que
ocorreram no mesmo.</p>
<blockquote>
<pre class="doctest-block">&gt;&gt;&gt; word = 'living'
&gt;&gt;&gt; cfdist['living'].samples()
['creature,', 'substance', 'soul.', 'thing', 'thing,', 'creature']
</pre>
</blockquote>
<p>We can set up a simple loop to generate text: we set an initial
context, picking the most likely token in that context as our next
word, and then using that word as our new context:</p>
<p>Podemos criar um ciclo simples para gerar texto: determinamos um contexto
inicial, selecionando o toquen mais provável para este contexto como a
nossa próxima palavra, e então utilizando esta palavra como nosso
novo contexto:</p>
<blockquote>
<pre class="doctest-block">&gt;&gt;&gt; word = 'living'
&gt;&gt;&gt; for i in range(20):
...     print word,
...     word = cfdist[word].max()
living creature that he said, I will not be a wife of the land of the land of the land
</pre>
</blockquote>
<p>Esta abordagem simplista para a geração de texto tende a prender-se em
ciclos repetitivos, como demostrado pelo texto gerado acima. Uma
abordagem mais avançada poderia escolher aleatoriamente cada palavra,
com as palavras mais freqüentes escolhidas um maior número de vezes.</p>
</div>
<div class="section">
<h1><a id="classes-de-palavras-e-partes-do-discurso" name="classes-de-palavras-e-partes-do-discurso">6&nbsp;&nbsp;&nbsp;Classes de palavras e partes-do-discurso</a></h1>
<p>Nas seções anteriores tratamos todas as palavras de forma muito
semelhante: qualquer coisa era ou não era um toquen. Porém, para muitas
aplicações, é importante que possamos distinguir entre <em>tipos
diferentes</em> de tokens. Por exemplo, podemos querer marcar explicitamente
que determinada string é um item lexical comum, que outra constitui
uma expressão numérica e que outra ainda é um sinal de pontuação. Além
disto, podemos querer distinguir entre os diferentes tipos de itens
lexicais: de fato, há uma longa tradição no campo da lingüística em 
classificarem-se as palavras em categorias chamadas <em>partes-do-discurso</em>.
Estas também podem ser chamadas de classes de palavras ou de
<em>categorias lexicais</em>. Exemplos familiares são <em>substantivo</em>, <em>verbo</em>,
<em>preposição</em>, <em>adjetivo</em> e <em>advérbio</em>. Nesta seção apresentaremos os
critérios padrão para a categorização de palavras desta forma,
discutindo as principais classes de palavras da língua inglesa.</p>
<div class="section">
<h2><a id="categorizando-palavras" name="categorizando-palavras">6.1&nbsp;&nbsp;&nbsp;Categorizando palavras</a></h2>
<p>Como decidimos a qual categoria uma palavra deve pertencer? Em
geral, os lingüistas valem-se de três tipos de avaliações para tomar
esta decisão: uma formal, uma sintática (ou distributiva) e uma
conceitualista (ou semântica). Uma avaliação <em>formal</em> é aquela que
analisa a estrutura interna de uma palavra. Por exemplo, <tt class="docutils literal"><span class="pre">-ness</span></tt> é
um sufixo que combina-se a um adjetivo para produzir um substantivo.
Exemplos são <tt class="docutils literal"><span class="pre">happy</span></tt> &gt; <tt class="docutils literal"><span class="pre">happiness</span></tt>, <tt class="docutils literal"><span class="pre">ill</span></tt> &gt; <tt class="docutils literal"><span class="pre">illness</span></tt>. <a class="footnote-reference" href="#id5" id="id4" name="id4">[2]</a>
Assim, ao encontrarmos uma palavra que termine em <tt class="docutils literal"><span class="pre">-ness</span></tt>, podemos
com grande probabilidade identificá-la como sendo um substantivo. &lt;/para&gt;</p>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label"><col></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4" name="id5">[2]</a></td><td>Utilizamos &gt; com o significado de 'deriva em'.</td></tr>
</tbody>
</table>
<p>Um critério sintático considera os contextos sintáticos nos quais uma
palavra pode ocorrer. Por exemplo, vamos assumir que já tenhamos
definido a categoria dos substantivos. Em um critério sintática para a
língua inglesa, podemos dizer que um adjetivo é aquela palavra que
ocorre imediatamente antes de um substantivo ou que segue as palavras 
<tt class="docutils literal"><span class="pre">be</span></tt> e <tt class="docutils literal"><span class="pre">very</span></tt>. De acordo com estes testes, <tt class="docutils literal"><span class="pre">near</span></tt> deveria ser
categorizada como um adjetivo:</p>
<ol class="arabic simple">
<li>the near window</li>
<li>The end is (very) near.</li>
</ol>
<p>Um exemplo conhecido de critério conceitualista é definir um substantivo
como o "nome de uma pessoa, lugar ou coisa". Dentro da lingüística moderna,
critérios nocionais para definição de classes de palavras têm sido vistos
com considerável desconfiança, principalmente devido à dificuldade de
se formalizar tais conceitos. Apesar disto, critérios conceitualistas são
a base de muitas de nossas intuições quanto a classes de palavras e nos
permitem estimar, com uma boa probabilidade de acerto, a categorização de
palavras em línguas que desconhecemos; isto é, se por exemplo soubermos apenas
que em holandês <tt class="docutils literal"><span class="pre">verjaardag</span></tt> significa o mesmo que a palavra inglesa
<tt class="docutils literal"><span class="pre">birthday</span></tt> ou a portuguesa <tt class="docutils literal"><span class="pre">aniversário</span></tt>, podemos supor que
<tt class="docutils literal"><span class="pre">verjaardag</span></tt> seja um substantivo também em holandês. Porém, é necessária
cautela: mesmo que possamos traduzir <tt class="docutils literal"><span class="pre">zij</span> <span class="pre">is</span> <span class="pre">van</span> <span class="pre">dag</span> <span class="pre">jarig</span></tt> como <tt class="docutils literal"><span class="pre">it's</span>
<span class="pre">her</span> <span class="pre">birthday</span> <span class="pre">today</span></tt> ou é <tt class="docutils literal"><span class="pre">hoje</span> <span class="pre">é</span> <span class="pre">o</span> <span class="pre">seu</span> <span class="pre">(dela)</span> <span class="pre">aniversário</span></tt>, a palavra
<tt class="docutils literal"><span class="pre">jarig</span></tt> é na verdade um adjetivo em holandês e não há uma eqüivalência
exata desta em inglês ou em português.</p>
<!-- http://www.askoxford.com/pressroom/archive/odelaunch/ -->
<p>Todas as línguas adquirem novos itens lexicais. A lista de palavras
que recentemente foi adicionada ao Oxford Dictionary of English inclui
<tt class="docutils literal"><span class="pre">cyberslacker,</span> <span class="pre">fatoush,</span> <span class="pre">blamestorm,</span> <span class="pre">SARS,</span> <span class="pre">cantopop,</span> <span class="pre">bupkis,</span> <span class="pre">noughties,</span>
<span class="pre">muggle</span></tt> e <tt class="docutils literal"><span class="pre">robata</span></tt>. Note que todas estas palavras são substantivos;
isto se reflete no fato dos substantivos serem considerados uma <em>classe
aberta</em>. Em contraste, as preposições são consideradas uma <em>classe fechada</em>,
ou seja, há um conjunto limitado de palavras que pertence a tal classe
(por exemplo, sempre em inglês, <tt class="docutils literal"><span class="pre">above,</span> <span class="pre">along,</span> <span class="pre">at,</span> <span class="pre">below,</span> <span class="pre">beside,</span> <span class="pre">between,</span>
<span class="pre">during,</span> <span class="pre">for,</span> <span class="pre">from,</span> <span class="pre">in,</span> <span class="pre">near,</span> <span class="pre">on,</span> <span class="pre">outside,</span> <span class="pre">over,</span> <span class="pre">past,</span> <span class="pre">through,</span>
<span class="pre">towards,</span> <span class="pre">under,</span> <span class="pre">up,</span> <span class="pre">with</span></tt>) e as mudanças ocorrem muito gradualmente
ao longo do tempo.</p>
<!-- Some word classes consist of a limited set of so-called
*function* words. Prepositions are one such class, comprising
items like etc.  These are called *closed classes*, in the sense
that although languages acquire new lexical items.  Content words
such as nouns are not limited in this way, and are continually
being extended with the invention of new words.  These are called
*open classes*. -->
</div>
<div class="section">
<h2><a id="classes-de-palavras-em-ingl-s" name="classes-de-palavras-em-ingl-s">6.2&nbsp;&nbsp;&nbsp;Classes de palavras em inglês</a></h2>
<p>Esta seção apresenta uma breve panorâmica sobre as classe de palavras
na língua inglesa. Leitores interessados em maiores detalhes são
aconselhados a consultar uma gramática da língua.</p>
<p>Linguists commonly recognize four major categories of open class words
in English, namely nouns, verbs, adjectives and adverbs.  Nouns
generally refer to people, places, things, or concepts, e.g.: <em>woman,
Scotland, book, intelligence</em>.  In the context of a sentence, nouns
can appear after determiners and adjectives, and can be the subject or
object of the verb:</p>
<p>Os lingüistas costumam reconhecer quatro categorias principais de
classes abertas de palavras em inglês: substantivos, verbos, adjetivos
e advérbios. Substantivos referem-se, geralmente, a pessoas, lugares,
coisas ou conceitos, como <tt class="docutils literal"><span class="pre">woman,</span> <span class="pre">Scotland,</span> <span class="pre">book,</span> <span class="pre">intelligence</span></tt>.
No contexto da sentença, os substantivos podem ser encontrados após
determinantes e adjetivos, e podem ser o sujeito ou objeto de um verbo:</p>
<table class="docutils" border="1">
<colgroup>
<col width="11%">
<col width="42%">
<col width="47%">
</colgroup>
<tbody valign="top">
<tr><td colspan="3">Padrões sintáticos envolvendo alguns substantivos</td>
</tr>
<tr><td>Palavra</td>
<td>Após um determinante</td>
<td>Sujeito de um verbo</td>
</tr>
<tr><td>woman</td>
<td><em>the</em> woman who I saw yesterday ...</td>
<td>the woman <em>sat</em> down</td>
</tr>
<tr><td>Scotland</td>
<td><em>the</em> Scotland I remember as a child ...</td>
<td>Scotland <em>has</em> five million people</td>
</tr>
<tr><td>book</td>
<td><em>the</em> book I bought yesterday ...</td>
<td>this book <em>recounts</em> the colonisation of Australia</td>
</tr>
<tr><td>intelligence</td>
<td><em>the</em> intelligence displayed by the child ...</td>
<td>Mary's intelligence <em>impressed</em> her teachers</td>
</tr>
</tbody>
</table>
<p>Em inglês, os substantivos podem ser morfologicamente complexos. Por exemplo,
palavras como <tt class="docutils literal"><span class="pre">books</span></tt> e <tt class="docutils literal"><span class="pre">women</span></tt> estão no plural. Como vimos
anteriormente, palavras terminadas com o sufixo <tt class="docutils literal"><span class="pre">-ness</span></tt> são substantivos
derivados de adjetivos, como <tt class="docutils literal"><span class="pre">happiness</span></tt> e <tt class="docutils literal"><span class="pre">illness</span></tt>. O sufixo <tt class="docutils literal"><span class="pre">-ment</span></tt>
é encontrado em alguns substantivos derivados de verbos, como
em <tt class="docutils literal"><span class="pre">government</span></tt> e <tt class="docutils literal"><span class="pre">establishment</span></tt>.</p>
<p>Os substantivos são geralmente divididos em <em>substantivos comuns</em> e
<em>substantivos próprios</em> (ou "nomes comuns" e "nomes próprios").
Substantivos próprios identificam índividuos ou entidades em particular, como
<tt class="docutils literal"><span class="pre">Moses</span></tt> e <tt class="docutils literal"><span class="pre">Scotland</span></tt>, enquanto substantivos comuns são todos os
restantes. Outra distinção importante existe entre <em>substantivos contáveis</em>
e <em>substantivos incontáveis</em> (ou "substantivos de massa"). Substantivos
contáveis são concebidos como entidades distintas que podem ser contadas,
como <tt class="docutils literal"><span class="pre">pig</span></tt> (por exemplo, <tt class="docutils literal"><span class="pre">one</span> <span class="pre">pig,</span> <span class="pre">two</span> <span class="pre">pigs,</span> <span class="pre">many</span> <span class="pre">pigs</span></tt>). Eles não
podem ocorrer com a palavra <tt class="docutils literal"><span class="pre">much</span></tt> (como em *``much pigs``).
Substantivos incontáveis, por outro lado, não são vistos como entidades
distintas (por exemplo, <tt class="docutils literal"><span class="pre">sand</span></tt>). Eles não podem ser pluralizados e não
podem ocorrer com numerais (por exemplo, *``two sands``, *``many sands``).
Por outro lado, eles podem ocorrer com <tt class="docutils literal"><span class="pre">much</span></tt> (como em <tt class="docutils literal"><span class="pre">much</span> <span class="pre">sand</span></tt>).</p>
<p>Verbos são palavras que descrevem eventos e ações, como <tt class="docutils literal"><span class="pre">fall</span></tt> e
<tt class="docutils literal"><span class="pre">eat</span></tt>. No contexto da sentença, verbos expressam a relação envolvendo
os referentes de um ou mais sintagmas nominais.</p>
<table class="docutils" border="1">
<colgroup>
<col width="10%">
<col width="24%">
<col width="66%">
</colgroup>
<tbody valign="top">
<tr><td colspan="3">Padrões sintáticos envolvendo alguns verbos</td>
</tr>
<tr><td>Palavra</td>
<td>Simples</td>
<td>Com modificadores e adjuntos (em itálico)</td>
</tr>
<tr><td>fall</td>
<td>Rome fell</td>
<td>Dot com stocks <em>suddenly</em> fell <em>like a stone</em></td>
</tr>
<tr><td>eat</td>
<td>Mice eat cheese</td>
<td>John ate the pizza <em>with gusto</em></td>
</tr>
</tbody>
</table>
<p>Verbos podem ser classificados de acordo com o número de argumentos
(geralmente sintagmas nominais) que os acompanham. A palavra <tt class="docutils literal"><span class="pre">fall</span></tt>
("cair") é <em>intransitiva</em>, requerendo apenas um argumento (a entidade
que cai). A palavra <tt class="docutils literal"><span class="pre">eat</span></tt> ("comer") é <em>transitiva</em>, requerendo dois
argumentos (o comedor e o comido). Outros verbos são mais complexos; por
exemplo, <tt class="docutils literal"><span class="pre">put</span></tt> ("colocar") requer três argumentos: o agente que está
realizando a ação de colocar, a entidade que está sendo colocada em algum
lugar e a sua localização final. O sufixo <tt class="docutils literal"><span class="pre">-ing</span></tt> é encontrado em
substantivos derivados de verbos, como <tt class="docutils literal"><span class="pre">the</span> <span class="pre">falling</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">leaves</span></tt>
(o que é conhecido por gerúndio).</p>
<p>Em inglês, verbos podem ser morfologicamente complexos. Por exemplo, o
<em>particípio presente</em> de um verbo termina em <tt class="docutils literal"><span class="pre">-ing</span></tt> e expressa a idéia
de estar em execução, de uma ação incompleta (como <tt class="docutils literal"><span class="pre">falling</span></tt> e
<tt class="docutils literal"><span class="pre">eating</span></tt>). O <em>particípio passado</em> de um verbo geralmente termina em
<tt class="docutils literal"><span class="pre">-ed</span></tt> e expressa a idéia de uma ação concluída (como <tt class="docutils literal"><span class="pre">fell</span></tt> e <tt class="docutils literal"><span class="pre">ate</span></tt>).</p>
<p>Duas outras importantes classes são os <em>adjetivos</em> e <em>advérbios</em>.
Adjetivos descrevem substantivos e podem ser utilizados como
modificadores (por exemplo, <tt class="docutils literal"><span class="pre">large</span></tt> em <tt class="docutils literal"><span class="pre">the</span> <span class="pre">large</span> <span class="pre">pizza</span></tt>) ou em
predicados (por exemplo, <tt class="docutils literal"><span class="pre">the</span> <span class="pre">pizza</span> <span class="pre">is</span> <span class="pre">large</span></tt>). Em inglês, adjetivos
podem ser morfologicamente complexos (como
<tt class="docutils literal"><span class="pre">fall&lt;subscript&gt;V&lt;/subscript&gt;+ing</span></tt> em <tt class="docutils literal"><span class="pre">the</span> <span class="pre">falling</span> <span class="pre">stocks</span></tt>).
Advérbios modificam verbos para especificar o tempo, o modo, o lugar
ou a direção do evento descrito pelo verbo (como <tt class="docutils literal"><span class="pre">quickly</span></tt> em
<tt class="docutils literal"><span class="pre">the</span> <span class="pre">stocks</span> <span class="pre">fell</span> <span class="pre">quickly</span></tt>). Advérbios também podem modificar
adjetivos (como <tt class="docutils literal"><span class="pre">really</span></tt> em <tt class="docutils literal"><span class="pre">Mary's</span> <span class="pre">teacher</span> <span class="pre">was</span> <span class="pre">really</span> <span class="pre">nice</span></tt>).</p>
<p>O inglês possui várias categorias de classes fechadas de palavras além
das preposições e cada dicionário e gramática as classifica de forma
diferente. A tabela abaixo fornece um exemplo de classes fechadas de
palavras, segundo a classificação utilizada no Brown Corpus. <a class="footnote-reference" href="#id7" id="id6" name="id6">[3]</a></p>
<table class="docutils footnote" frame="void" id="id7" rules="none">
<colgroup><col class="label"><col></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id6" name="id7">[3]</a></td><td>Note que as tags para partes-do-discurso podem ser apresentadas
tanto em letra maiúscula quando em letra minúscula -- não há
nenhuma significação resultante desta diferença.</td></tr>
</tbody>
</table>
<table class="docutils" border="1">
<colgroup>
<col width="2%">
<col width="21%">
<col width="77%">
</colgroup>
<tbody valign="top">
<tr><td colspan="3">Algumas classes de palavras fechadas do inglês, com as tags do Brown Tag</td>
</tr>
<tr><td>ap</td>
<td>determiner/pronoun, post-determiner</td>
<td>many other next more last former little several enough most least only very few fewer past same</td>
</tr>
<tr><td>at</td>
<td>article</td>
<td>the an no a every th' ever' ye</td>
</tr>
<tr><td>cc</td>
<td>conjunction, coordinating</td>
<td>and or but plus &amp; either neither nor yet 'n' and/or minus an'</td>
</tr>
<tr><td>cs</td>
<td>conjunction, subordinating</td>
<td>that as after whether before while like because if since for than 
until so unless though providing once lest till whereas whereupon 
supposing albeit then</td>
</tr>
<tr><td>in</td>
<td>preposition</td>
<td>of in for by considering to on among at through with under into regarding than since despite ...</td>
</tr>
<tr><td>md</td>
<td>modal auxiliary</td>
<td>should may might will would must can could shall ought need wilt</td>
</tr>
<tr><td>pn</td>
<td>pronoun, nominal</td>
<td>none something everything one anyone nothing nobody everybody everyone anybody anything someone no-one nothin'</td>
</tr>
<tr><td>ppl</td>
<td>pronoun, singular, reflexive</td>
<td>itself himself myself yourself herself oneself ownself</td>
</tr>
<tr><td>pp$</td>
<td>determiner, possessive</td>
<td>our its his their my your her out thy mine thine</td>
</tr>
<tr><td>pp$$</td>
<td>pronoun, possessive</td>
<td>ours mine his hers theirs yours</td>
</tr>
<tr><td>pps</td>
<td>pronoun, personal, nom, 3rd pers sng</td>
<td>it he she thee</td>
</tr>
<tr><td>ppss</td>
<td>pronoun, personal, nom, not 3rd pers sng</td>
<td>they we I you ye thou you'uns</td>
</tr>
<tr><td>wdt</td>
<td>WH-determiner</td>
<td>which what whatever whichever</td>
</tr>
<tr><td>wps</td>
<td>WH-pronoun, nominative</td>
<td>that who whoever whosoever what whatsoever</td>
</tr>
</tbody>
</table>
</div>
<div class="section">
<h2><a id="conjunto-de-tags-das-partes-do-discurso" name="conjunto-de-tags-das-partes-do-discurso">6.3&nbsp;&nbsp;&nbsp;Conjunto de tags das partes-do-discurso</a></h2>
<p>Most part-of-speech tag sets make use of the same basic categories,
such as noun, verb, adjective, and preposition. However, tag sets
differ both in how finely they divide words into categories; and in
how they define their categories. For example, <tt class="docutils literal"><span class="pre">is</span></tt> might be tagged
as a verb in one tag set; but as a distinct form of <tt class="docutils literal"><span class="pre">to</span> <span class="pre">be</span></tt> in
another tag set -- in fact, we just observed the latter situation
in the Brown Corpus tag set.  This variation in tag sets is
unavoidable, since part-of-speech tags are used in different ways for
different tasks. In other words, there is no one 'right way' to assign
tags, only more or less useful ways, depending on one's goals.</p>
<p>A maior parte dos conjuntos de tags ("tag sets") utiliza as mesmas
categorias básicas, como substantivo, verbo, adjetivo e preposição. Estes
conjuntos de tags diferem, porém, tanto na extensão com que as palavras
são divididas em categorias quanto na forma como estas últimas são
definifas. Por exemplo, <tt class="docutils literal"><span class="pre">is</span></tt> pode receber a tag de verbo em um determinado
conjunto mas ser considerado uma forma distinta de <tt class="docutils literal"><span class="pre">to</span> <span class="pre">be</span></tt> em
outro -- de fato, obverservados este último caso antes, no conjunto de tags
do Brown Corpus.  Esta variação nos conjuntos de tags é inevitável,
já que as tags de partes-do-discurso são utilizadas de formas diferentes
para diferentes tarefas. Em outras palavras, não há uma "forma correta" de
se atribuir tags, apenas há formas mais e menos apropriadas, dependendo
dos objetivos de cada um.</p>
<!-- <note><para> Há varios conjuntos de tags para partes-do-discurso
utilizados amplamente, porque há diferentes esquemas para a classificação
das palavras (em função dos diferentes pesos dados aos critérios
formais, sintáticos ou nocionais), e porque as diferentes tarefas
de processamento exigem classificações mais precisas ou
gerais.</para></note> -->
<p>Observe como o processo de tagging simultaneamente descarta algumas
distinções (como por exemplo a identidade lexical, que é perdida
quando todos os pronomes pessoais recebem a mesma tag <tt class="docutils literal"><span class="pre">prp</span></tt>) e
introduz novas, removendo ambigüidades (como por exemplo <tt class="docutils literal"><span class="pre">deal</span></tt> que
recebe a tag <tt class="docutils literal"><span class="pre">vb</span></tt> ou a tag <tt class="docutils literal"><span class="pre">nn</span></tt>). Esta forma de operação facilita
a classificação e predição. Observe que quando introduzimos distinções
mais específicas no conjunto de tags obtemos uma melhor informação quanto
ao contexto lingüístico, mas é necessário um trabalho maior para
classificar cada toquen (há um numero maior de tags possíveis entre
as quais escolher a correta). Da mesma forma, com um menor nível de
distinção entre as tags será mais simples classificar cada toquen, mas
uma quantidade menor de informações referentes ao contexto
permencerá disponível.</p>
<p>In this tutorial, we will use the following tags: <tt class="docutils literal"><span class="pre">at</span></tt> (article)
<tt class="docutils literal"><span class="pre">nn</span></tt> (Noun), <tt class="docutils literal"><span class="pre">vb</span></tt> (Verb), <tt class="docutils literal"><span class="pre">jj</span></tt> (Adjective), <tt class="docutils literal"><span class="pre">in</span></tt>
(Preposition), <tt class="docutils literal"><span class="pre">cd</span></tt> (Number), and <tt class="docutils literal"><span class="pre">end</span></tt> (Sentence-ending
punctuation).  As we mentioned, this is a radically simplified version
of the Brown Corpus tag set, which in its entirety has 87 basic tags
plus many combinations.</p>
<p>Neste tutorial, usaremos as seguintes tags: <tt class="docutils literal"><span class="pre">at</span></tt> (artigo), <tt class="docutils literal"><span class="pre">nn</span></tt>
(substantivo), <tt class="docutils literal"><span class="pre">vb</span></tt> (verbo), <tt class="docutils literal"><span class="pre">jj</span></tt> (adjetivo), <tt class="docutils literal"><span class="pre">in</span></tt> (preposição),
<tt class="docutils literal"><span class="pre">cd</span></tt> (numeral), e <tt class="docutils literal"><span class="pre">end</span></tt> (pontuação de final de sentença). Como dissemos,
esta é uma versão radicalmente simplificada do conjunto de tags do
Brown Corpus, o qual em versão integral apresenta 87 tags de base além
de várias combinações.</p>
</div>
</div>
<div class="section">
<h1><a id="representando-toquens-e-corpora-com-tags" name="representando-toquens-e-corpora-com-tags">7&nbsp;&nbsp;&nbsp;Representando toquens e corpora com tags</a></h1>
<p>Nas seções anteriores foi discutida a natureza e a utilização das tags
no processamento lingüístico. Nesta seção introduziremos a representação
computacional de tags. Primeiro iremos considerar os tokens marcados
individualmente com tags, mostrando de que forma eles são criados e como
podem ser acessados; finalmente nos interessaremos pelos corpora
com tags.</p>
<p>Por convenção, um toquen com tag é representado utilizando-se um
tuple do Python:</p>
<pre class="literal-block">&gt;&gt;&gt; tok = ('fly', 'nn')
&gt;&gt;&gt; tok
('fly', 'nn')
</pre>
<p>Podemos acessar as propriedades deste toquen da forma costumeira, como
demonstrado abaixo:</p>
<blockquote>
<pre class="doctest-block">&gt;&gt;&gt; print tok[0]
fly
&gt;&gt;&gt; print tok[1]
nn
</pre>
</blockquote>
<p>Vários corpora extensos (como o Brown Corpus e a partes do Wall Street
Journal) passaram por um processo de tagging manual, recebendo tags
referentes às partes-do-discurso. Antes que possamos utilizar estes
corpora, é necessário que os carreguemos a partir dos arquivos que os
contêem e que os toquenizemos.</p>
<p>Textos com tags são geralmente armazenados em arquivos como seqüências de
toquens separados por espaços em branco, onde cada toquen é representado na
forma <tt class="docutils literal"><span class="pre">texto/tag</span></tt>, como ilustrado abaixo em uma seqüência extraída do
Brown Corpus:</p>
<pre class="literal-block">The/at grand/jj jury/nn commented/vbd on/in a/at number/nn of/in
other/ap topics/nns ,/, among/in them/ppo the/at Atlanta/np and/cc
Fulton/np-tl County/nn-tl purchasing/vbg departments/nns which/wdt it/pps
said/vbd ``/`` are/ber well/ql operated/vbn and/cc follow/vb generally/rb
accepted/vbn practices/nns which/wdt inure/vb to/in the/at best/jjt
interest/nn of/in both/abx governments/nns ''/'' ./.
</pre>
<p>É possível utilizar o módulo <tt class="docutils literal"><span class="pre">nltk_lite.corpora</span></tt> para ler e toquenizar
dados a partir de corpus com tags, como vimos acima.</p>
<p>Eis outro exemplo que constroi toquens a partir de uma string:</p>
<pre class="literal-block">&gt;&gt;&gt; sent = """
... John/nn saw/vb the/at book/nn on/in the/at table/nn ./end  He/nn sighed/vb ./end
... """
&gt;&gt;&gt; from nltk_lite.tag import tag2tuple
&gt;&gt;&gt; for t in tokenize.whitespace(sent):
...     print tag2tuple(t),
('John', 'nn') ('saw', 'vb') ('the', 'at') ('book', 'nn') ('on', 'in') ('the', 'at') ('table', 'nn') ('.', 'end') ('He', 'nn') ('sighed', 'vb') ('.', 'end')
</pre>
</div>
<div class="section">
<h1><a id="mais-aplica-es" name="mais-aplica-es">8&nbsp;&nbsp;&nbsp;Mais aplicações</a></h1>
<p>Agora que podemos acessar um texto com tags, é possível realizar uma
variedade de úteis tarefas de processamento. Iremos considerar aqui
apenas duas: estimar a tag da parte-do-discurso de uma palavra e explorar
a distribuição de freqüência de verbos modais de acordo com o gênero de
cada texto (em língua inglesa).</p>
<div class="section">
<h2><a id="classifica-o-autom-tica-de-palavras" name="classifica-o-autom-tica-de-palavras">8.1&nbsp;&nbsp;&nbsp;Classificação automática de palavras</a></h2>
<p>Um corpus com tags pode ser utilizado para <em>treinar</em> um classificador
simples que pode ser utilizado para estimar a tag de palavras ainda não
classificadas. Para cada palavra, iremos contar o número de vezes que
esta recebe cada tag. Por exemplo, a palavra <tt class="docutils literal"><span class="pre">deal</span></tt> recebe 89 vezes a tag
<tt class="docutils literal"><span class="pre">nn</span></tt> e 41 vezes a tag <tt class="docutils literal"><span class="pre">vb</span></tt>. Baseados nisso, se tivessemos de estimar uma
tag para a palavra <tt class="docutils literal"><span class="pre">deal</span></tt>, escolheríamos <tt class="docutils literal"><span class="pre">nn</span></tt>, com um índice de acerto
de mais de dois terços. O programa listado a seguir executa esta tarefa
de tagging quando treinado sobre a seção "g" do Brown Corpus (a chamada
<em>belles lettres</em>, literatura criativa considerada em função de seu
conteúdo estético).</p>
<blockquote>
<pre class="doctest-block">&gt;&gt;&gt; cfdist = ConditionalFreqDist()
&gt;&gt;&gt; for sentence in brown.tagged('g'):
...     for token in sentence:
...         word = token[0]
...         tag = token[1]
...         cfdist[word].inc(tag)
&gt;&gt;&gt; for word in "John saw 3 polar bears".split():
...     print word, cfdist[word].max()
John np
saw vbd
3 cd-tl
polar jj
bears vbz
</pre>
</blockquote>
<p>Note que <tt class="docutils literal"><span class="pre">bears</span></tt> recebeu incorretamente a tag de "forma na terceira
pessoa do singular de um verbo", pois esta palavra é encontrada mais
freqüentemente como verbo que como substantivo na literatura de
fins estéticos.</p>
<p>Um problema desta abordagem é o fato dela resultar em um modelo extenso,
com uma entrada para cada combinação possível de palavra e tag. Para
certas tarefas, é possível construir modelos razoavelmente bem sucedidos
que, em comparação, são muito menores. Por exemplo, vamos tentar estimar
se uma palavra é um verbo, um substantivo ou um adjetivo analisando
apenas sua letra final. Podemos fazer isto da seguinte forma:</p>
<blockquote>
<pre class="doctest-block">&gt;&gt;&gt; tokens = []
&gt;&gt;&gt; for sent in brown.tagged('g'):
...     for (word,tag) in sent:
...         if tag in ['nn', 'jj'] and len(word) &gt; 3:
...             char = word[-1]
...             tokens.append((char,tag))
&gt;&gt;&gt; split = len(tokens)*9/10
&gt;&gt;&gt; train, test = tokens[:split], tokens[split:]
&gt;&gt;&gt; cfdist = ConditionalFreqDist()
&gt;&gt;&gt; for (char,tag) in train:
...     cfdist[char].inc(tag)
&gt;&gt;&gt; correct = total = 0
&gt;&gt;&gt; for (char,tag) in test:
...     if tag == cfdist[char].max():
...         correct += 1
...     total += 1
&gt;&gt;&gt; print correct*100/total
71
</pre>
</blockquote>
<p>Este resultado de 71% é marginalmente melhor que o resultado de 65% que
obteríamos ao atribuirmos a tag <tt class="docutils literal"><span class="pre">nn</span></tt> a todas as palavras. Podemos
inspecionar o modelo para ver qual tag é atribuída a cada palavra em função
de sua letra final. Desta forma, aprendemos que as palavras terminadas em
<tt class="docutils literal"><span class="pre">c</span></tt> ou <tt class="docutils literal"><span class="pre">l</span></tt> têm uma probabilidade maior de serem adjetivos do que
substantivos.</p>
<blockquote>
<pre class="doctest-block">&gt;&gt;&gt; print [(c, cfdist[c].max()) for c in cfdist.conditions()]
[('%', 'nn'), ("'", None), ('-', 'jj'), ('2', 'nn'), ('5', 'nn'), ('A', 'nn'), ('D', 'nn'), ('O', 'nn'), ('S', 'nn'), ('a', 'nn'), ('c', 'jj'), ('b', 'nn'), ('e', 'nn'), ('d', 'nn'), ('g', 'nn'), ('f', 'nn'), ('i', 'nn'), ('h', 'nn'), ('k', 'nn'), ('m', 'nn'), ('l', 'jj'), ('o', 'nn'), ('n', 'nn'), ('p', 'nn'), ('s', 'nn'), ('r', 'nn'), ('u', 'nn'), ('t', 'nn'), ('w', 'nn'), ('y', 'nn'), ('x', 'nn'), ('z', 'nn')]
</pre>
</blockquote>
</div>
<div class="section">
<h2><a id="explorando-g-neros-textuais" name="explorando-g-neros-textuais">8.2&nbsp;&nbsp;&nbsp;Explorando gêneros textuais</a></h2>
<p>Agora que podemos carregar uma quantidade significativa de texto com
tags, é possível processá-lo e extrair deste informações de interesse. O
código a seguir interage nos quinze gêneros do Brown Corpus (acessados
por meio do método <tt class="docutils literal"><span class="pre">brown.groups()</span></tt>). O material para cada gênero está
armazenado em um conjunto de arquivos (acessados por meio do método
<tt class="docutils literal"><span class="pre">brown.items()</span></tt>). Estes são toquenizados em série; a etepa seguinte é
verificar se cada toquen possui a tag <tt class="docutils literal"><span class="pre">md</span></tt>. Para cada uma destas palavras,
incrementamos a cotnagem. Este método utiliza uma distribução de freqüência
condicional, na qual a condição é o gênero sendo analisado e o evento
o verbo modal.</p>
<blockquote>
<pre class="doctest-block">&gt;&gt;&gt; cfdist = ConditionalFreqDist()
&gt;&gt;&gt; for genre in brown.items:                  # cada gênero
...     for sent in brown.tagged(genre):       # cada sentença
...         for (word,tag) in sent:            # cada toquen com tag
...             if tag == 'md':                # encontrado um verbo modal
...                  cfdist[genre].inc(word.lower())
</pre>
</blockquote>
<p>A distribuição condicional de freqüência nada mais é que um mapeamento
entre cada gênero e a distribuição dos verbos modais em cada gênero. O
fragmento de código a seguir identifica um pequeno conjunto de verbos
modais de interesse e processa a estrutura de dados para exibir como
resultado as contagens requeridas.</p>
<blockquote>
<pre class="doctest-block">&gt;&gt;&gt; modals = ['can', 'could', 'may', 'might', 'must', 'will']
&gt;&gt;&gt; print "%-40s" % 'Genre', ' '.join([("%6s" % m) for m in modals])
Genre                                       can  could    may  might   must   will
&gt;&gt;&gt; for genre in cfdist.conditions():    # gera colunas
...     print "%-40s" % brown.item_name[genre],
...     for modal in modals:
...         print "%6d" % cfdist[genre].count(modal),
...     print
press: reportage                             94     86     66     36     50    387
press: reviews                               44     40     45     26     18     56
press: editorial                            122     56     74     37     53    225
skill and hobbies                           273     59    130     22     83    259
religion                                     84     59     79     12     54     64
belles-lettres                              249    216    213    113    169    222
popular lore                                168    142    165     45     95    163
miscellaneous: government &amp; house organs    115     37    152     13     99    237
fiction: general                             39    168      8     42     55     50
learned                                     366    159    325    126    202    330
fiction: science                             16     49      4     12      8     16
fiction: mystery                             44    145     13     57     31     17
fiction: adventure                           48    154      6     58     27     48
fiction: romance                             79    195     11     51     46     43
humor                                        17     33      8      8      9     13
</pre>
</blockquote>
<p>Há alguns padrões interessantes nesta tabela. Por exemplo, compare as
colunas para literatura governamental ("miscellaneous: government &amp;
house organs") e literatura de aventura ("fiction: adventure"); a primeira
é dominada pelo uso de <tt class="docutils literal"><span class="pre">can,</span> <span class="pre">may,</span> <span class="pre">must</span></tt> e <tt class="docutils literal"><span class="pre">will</span></tt> enquanto a última é
caracterizada pelo uso de <tt class="docutils literal"><span class="pre">could</span></tt> e  <tt class="docutils literal"><span class="pre">might</span></tt>. Com algum esforço adicional
é possível estimar o gênero de cada novo texto automaticamente, em função de
sua distribuição de verbos modais.</p>
<p>Agora que vimos como toquens com tags e corpora com tags são criados e
acessados, estamos prontos para dar uma olhada na categorização automática
de palavras.</p>
</div>
</div>
<div class="section">
<h1><a id="leituras-adicionais" name="leituras-adicionais">9&nbsp;&nbsp;&nbsp;Leituras adicionais</a></h1>
<p>John Hopkins Center for Language and Speech Processing, 1999
Summer Workshop on Normalization of Non-Standard Words: Final Report
<a class="reference" href="http://www.clsp.jhu.edu/ws99/projects/normal/report.pdf">http://www.clsp.jhu.edu/ws99/projects/normal/report.pdf</a></p>
<p>Glossário do SIL de termos lingüísticos:
<a class="reference" href="http://www.sil.org/linguistics/GlossaryOfLinguisticTerms/">http://www.sil.org/linguistics/GlossaryOfLinguisticTerms/</a></p>
<p>Language Files: Materials for an Introduction to Language and
Linguistics (Eighth Edition), The Ohio State University Department of
Linguistics, <a class="reference" href="http://www.ling.ohio-state.edu/publications/files/">http://www.ling.ohio-state.edu/publications/files/</a></p>
</div>
<div class="section">
<h1><a id="exerc-cios" name="exerc-cios">10&nbsp;&nbsp;&nbsp;Exercícios</a></h1>
<p>#. Acessar e tokenizar um arquivo de texto: Obter algum texto puro (por
exemplo, visitando uma página da web e salvando seu conteúdo como
texto puro) e armazenar no arquivo 'corpus.txt'.</p>
<blockquote>
<p>#. Utilizando os métodos <tt class="docutils literal"><span class="pre">open()</span></tt> e <tt class="docutils literal"><span class="pre">read()</span></tt>, carregar o texto em uma
variável de tipo string e exibir seu conteúdo.</p>
<p>#. Now, initialize a new token with <tt class="docutils literal"><span class="pre">Token()</span></tt>, using this text.
Tokenize the text with <tt class="docutils literal"><span class="pre">WhitespaceTokenizer</span></tt>, and specify that the
result should be stored in the <tt class="docutils literal"><span class="pre">WORDS</span></tt> property.  Print the
result.</p>
<p>#. Agora, inicialize um novo token com <tt class="docutils literal"><span class="pre">Token()</span></tt>, utilizando este
texto. Toquenize o texto com o <tt class="docutils literal"><span class="pre">WhitespaceTokenizer</span></tt> e especifique que
o resultado deverá ser armazenado na propriedade <tt class="docutils literal"><span class="pre">WORDS</span></tt>. Exiba o
resultado.</p>
<p>#. A seguir, calcule o numero de toquens utilizando a função <tt class="docutils literal"><span class="pre">len()</span></tt> e
exiba o resultado.</p>
<p>#. Finally, discuss shortcomings of this method for tokenizing text.
In particular, identify any material which has not been correctly
tokenized.  (You may need to look for a more complex text.)</p>
<p>#. Finalmente, discuta as falhas deste método de toquenização de textos.
Em especial, identifique qualquer conteúdo que não tenha sido
corretamente toquenizado (talvez seja necessário utilizar algum texto
mais complexo).</p>
</blockquote>
<ol class="arabic simple">
<li>Toquenizar um texto utilizando expressões regulares:
Obtenha algum texto puro (por exemplo, visitando uma página da web e
salvando seu conteúdo como texto puro) e armazene-o no arquivo
'corpus.txt' para que possas responder às questões seguintes.</li>
</ol>
<blockquote>
<p>#. Processadores de texto normalmente dividem em sílabas as palavras
ao final das linhas (nas chamadas "quebra de linha"). Quando um documento
processado por estes programas é convertido para texto puro, estas partes
geralmente não são recombinadas. É fácil reconhecer estes casos procurando
na web por palavras divididas, como <tt class="docutils literal"><span class="pre">depart-</span> <span class="pre">ment</span></tt>. Crie um
<tt class="docutils literal"><span class="pre">RegexpTokenizer</span></tt> que trate este tipo de palavra quebrada como um
token único.</p>
<p>#. Considere o seguinte título de um livro: <em>This Is the Beat Generation:
New York-San Francisco-Paris</em> ("Esta é a Geração Beat: Nova Iorque-São
Francisco-Paris"). O que seria necessário para poder toquenizar este tipo
de string de tal forma que cada nome de cidade fosse armazenado como
um único toquen?</p>
</blockquote>
<ol class="arabic simple">
<li>Concordância:
Escreva uma função que receba uma palavra como argumento, e que procure
no Brown Corpus por ocorrências desta palavra. Para cada ocorrncia,
gere uma linha de texto que contenha a palavra em questão em seu centro.</li>
<li>Lei de Zipf:
Seja <em>f(w)</em> a freqüência de uma palavra <em>w</em> em um texto livre. Suponha
que todas as palavras do texto estão classificadas de acordo com suas
freqüências, da mais freqüênte à menos freqüente. A Lei de Zipf diz que
a freqüência de um tipo de palavra é inversamente proporcional à sua
classificação (ou seja, f*r=k, para alguma constante k). Por exemplo, a
quinquagésima (50a) palavra mais comum deverá ocorrer três vezes mais que
a centésimaquinquagésima (150a).</li>
</ol>
<blockquote>
<p>#. Escreva uma função em Python para processar um longo texto e plote a
freqüência das palavras em relação à sua classificação utilizando o
módulo nltk_lite.draw.plot. És capaz de confirmar a Lei de Zipf?
(Sugestão: é conveniente ajustar os eixos para log-log) O que ocorre nas
extremidades das linhas plotadas?</p>
<p>#. Gere um texto aleatório, por exemplo utilizando
<tt class="docutils literal"><span class="pre">random.choice("abcdefg</span> <span class="pre">")</span></tt>, lembrando de incluir o caractere de
espaço. Utilize o operador de concatenação de strigns para acumular os
caracteres em uma (realmente) longa string. A seguir, toquenize esta
string e gere um gráfico Zipf como acima, comparando os dois gráficos.
O que pode-se deduzir a respeito da Lei de Zipf a partir disto?</p>
</blockquote>
<p>#. Trabalhando com texto com tags: Escreva um programa que carregue o
Brown Corpus e, dada uma palavra, liste todas as tags possíveis para
esta e suas contagens de freqüência. Por exemplo, para a palavra <tt class="docutils literal"><span class="pre">strike</span></tt>
o programa deverá gerar: <tt class="docutils literal"><span class="pre">[('nn',</span> <span class="pre">25),</span> <span class="pre">('vb',</span> <span class="pre">21)]</span></tt>. (Sugestão: esta
tarefa envolve reverter e ordenar a lista de tuples que estará na
forma <tt class="docutils literal"><span class="pre">[(21,</span> <span class="pre">'vb'),</span> <span class="pre">(25,</span> <span class="pre">'nn')]</span></tt>. Para converter estas listas à forma
requerida, use <tt class="docutils literal"><span class="pre">word_freq</span> <span class="pre">=</span> <span class="pre">[(y,x)</span> <span class="pre">for</span> <span class="pre">(x,y)</span> <span class="pre">in</span> <span class="pre">freq_word]</span></tt>.)</p>
<blockquote>
<p>#. Use seu programa para exibir as tags e suas freqüências para as
seguintes palavras: <tt class="docutils literal"><span class="pre">can,</span> <span class="pre">fox,</span> <span class="pre">get,</span> <span class="pre">lift,</span> <span class="pre">like,</span> <span class="pre">but,</span> <span class="pre">frank,</span> <span class="pre">line,</span>
<span class="pre">interest</span></tt>. Assegure-se de que você conhece o significado de cada uma
das tags mais freqüentes.</p>
<p>#. Escreva um programa para encontrar as 20 palavras que possuem as
maiores variedades de tags.</p>
<p>#. Escolha palavras que podem ser tanto substantivos quanto verbos
(como <tt class="docutils literal"><span class="pre">deal</span></tt>). Tente adivinhar qual é a tag mais provável para cada
palavra e confira se você está certo.</p>
</blockquote>
<p>#. Prevendo a próxima palavra: o programa para previsão de palavras que
vimos neste capítulo prende-se rapidamente em ciclos. Modifique o programa
de forma que cada palavra seja escolhida aleatoriamente entre uma lista
das <em>n</em> palavras mais prováveis no contexto dado. (Sugestão: armazene as
<em>n</em> palavras mais prováveis em uma lista <tt class="docutils literal"><span class="pre">lwords</span></tt> e então escolha
aleatoriamente uma palavra a partir desta lista com o método
<tt class="docutils literal"><span class="pre">random.choice()</span></tt>)</p>
<blockquote>
<p>#. Escolha um gênero específico, como uma seção do Brown Corpus, uma
tradução do livro do Gênesis ou um dos corpora de grupos de
discussão ("newsgroups"). Treine seu sistema neste corpus e faça-o
gerar texto aleatoriamente. Você pode experimentar com diferentes
palavras iniciais. O texto resultante é intelegível? Examine os pontos
fortes e fracos deste método para geração aleatória de texto.</p>
<p>#. Repita a experiência com diferentes gêneros e com diferentes
quantidades de dados para treinamento. O que você pode observar?</p>
<p>#. Agora, treine seu sistema utilizando dois gêneros distintos e
experimente com a geração de texto de um gênero híbrido. Como na
questão anterior, discuta sobre suas observações.</p>
</blockquote>
<p>#. Classificação automática de palavras: o programa para a classificação
de palavras como substantivos ou adjetivos obteve um índice de acerto de
71%. Vamos tentar criar condições melhores para obter um sistema com
índice de acerto de 80% ou mais.</p>
<blockquote>
<p>#. Considere a alternativa de utilizar um sufixo mais longo, como por
exemplo os últimos dois ou três caracteres. O que acontece com o
desempenho? Quais sufixos são reconhecidos como pertinentes aos
adjetivos?</p>
<p>#. Explore outras alternativas, como utilizar um comprimento variável
de prefixos, o comprimento das próprias palavras ou o número de vogais
em uma palavra.</p>
<p>#. Por último, combine as múltiplas condições em um tuple e explore
quais combinações de condições fornecem os melhores resultados.</p>
</blockquote>
<p>#. Escreva um programa para implementar um ou mais testes de legibilidade
(<a class="reference" href="http://en.wikipedia.org/wiki/Readability">http://en.wikipedia.org/wiki/Readability</a>, em inglês).</p>
<p>#. Projete um algoritmo que encontre as frases estatisticamente improváveis
de uma coleção de documentos.
<a class="reference" href="http://www.amazon.com/gp/search-inside/sipshelp.html/">http://www.amazon.com/gp/search-inside/sipshelp.html/</a></p>
<hr class="docutils">
<p><a class="reference" href="http://nltk.sourceforge.net/">NLTK</a></p>
</div>
</div>


</body></html>